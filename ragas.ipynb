{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain_community.vectorstores.pinecone import Pinecone\n",
        "import pinecone\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from typing import List\n",
        "from stock_assistant.config.settings import OPENAI_API_KEY, PINECONE_API_KEY\n",
        "import os\n",
        "from langchain_core.messages import SystemMessage, AIMessage\n\n",
        "load_dotenv()\n",
        "# print(os.getenv(\"OPENAI_API_KEY\"))\n\n",
        "pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    api_key=OPENAI_API_KEY\n",
        ")\n",
        "vector_store = Pinecone.from_existing_index(\"stockrag\", embeddings)\n\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0.7,\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    api_key=OPENAI_API_KEY\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from typing import List\n",
        "class EnhancedRAGSystem:\n",
        "    def __init__(self, vector_store, llm, prompt):\n",
        "        self.vector_store = vector_store\n",
        "        # Store the prompt template separately\n",
        "        self.prompt_template = prompt\n",
        "        # Create the chain\n",
        "        self.chain = prompt | llm\n\n",
        "    def get_relevant_docs(self, question: str, k: int = 3) -> List:\n",
        "        \"\"\"Get relevant documents from vector store\"\"\"\n",
        "        if not isinstance(question, str):\n",
        "            question = str(question)\n",
        "        # Fix: This line was incorrectly indented in your code\n",
        "        results = self.vector_store.similarity_search(\n",
        "            query=question,\n",
        "            k=k\n",
        "        )\n",
        "        return results\n\n",
        "    def generate_response(self, question: str, context_docs: List) -> str:\n",
        "        \"\"\"Generate a response using the LLM chain\"\"\"\n",
        "        # Combine context from all relevant documents\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in context_docs])\n\n",
        "        input_dict = {\n",
        "            \"context\": context,\n",
        "            \"question\": question\n",
        "        }\n\n",
        "        # Generate response\n",
        "        response = self.chain.invoke(input_dict)\n\n",
        "        return response.content if hasattr(response, 'content') else str(response)\n\n",
        "    def process_query(self, question: str, k: int = 3) -> dict:\n",
        "        \"\"\"Process a query end-to-end\"\"\"\n",
        "        try:\n",
        "            # Get relevant documents\n",
        "            relevant_docs = self.get_relevant_docs(question, k)\n\n",
        "            # Generate response\n",
        "            response = self.generate_response(question, relevant_docs)\n\n",
        "            # Prepare source information\n",
        "            sources = [\n",
        "                {\n",
        "                    \"page\": doc.metadata.get(\"page\", \"Unknown\"),\n",
        "                    \"source\": doc.metadata.get(\"source\", \"Unknown\")\n",
        "                }\n",
        "                for doc in relevant_docs\n",
        "            ]\n\n",
        "            return {\n",
        "                \"response\": response,\n",
        "                \"sources\": sources,\n",
        "                \"success\": True\n",
        "            }\n\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"response\": f\"Error processing query: {str(e)}\",\n",
        "                \"sources\": [],\n",
        "                \"success\": False\n",
        "            }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RESPONSE_TEMPLATE = \"\"\"\n",
        "You are a helpful financial advisor assistant. Using the provided context, answer the user's question in a clear, concise, and informative way.\n",
        "If the information in the context is not sufficient, say so.\n\n",
        "Context from documents:\n",
        "{context}\n\n",
        "User Question: {question}\n\n",
        "Please provide a well-structured response that:\n",
        "1. Directly answers the question\n",
        "2. Includes relevant examples or explanations where appropriate\n",
        "3. Highlights any important caveats or considerations\n\n",
        "Response:\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "response_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=RESPONSE_TEMPLATE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rag_system = EnhancedRAGSystem(vector_store, llm, response_prompt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "result = rag_system.process_query(str(\"what is an initial public offer\"))\n",
        "relevant_docs = rag_system.get_relevant_docs(str(\"what are market orders and limit order in tradin\"))\n",
        "content = result[\"response\"]\n",
        "additional_kwargs = {\"sources\": result[\"sources\"]}\n",
        "print(content)\n",
        "print(additional_kwargs)\n",
        "print(relevant_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "questions = [\n",
        "    \"what are the dos for investing in mutual fund schemes\",\n",
        "    \"give me 2 mantras to wise investing\",\n",
        "    \"what are market orders and limit order in trading\",\n",
        "    \"what is an initial public offer\"\n",
        "]\n",
        "\n",
        "answers = [    \n",
        "    \"The DOs for investing in mutual fund schemes include: 1. Reading the offer document carefully before investing to understand the main features, risks, expenses, and track record of the scheme...\"\n",
        "    \"Two mantras to wise investing are: 1. Follow life-cycle investing: As you age, your risk tolerance should change...\"\n",
        "    \"Market orders and limit orders are two common types of orders in trading...\"\n",
        "    \"An Initial Public Offering (IPO) is when a previously unlisted company offers its shares to the public for the first time...\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datasets import Dataset\n",
        "from ragas import evaluate, EvaluationDataset\n",
        "from typing import List\n\n",
        "def handle_rag_evaluation():\n",
        "    dataset = []\n\n",
        "    for query, reference in zip(questions, answers):\n",
        "        relevant_docs: List = rag_system.get_relevant_docs(query)\n\n",
        "        contexts = relevant_docs\n",
        "        if relevant_docs and not isinstance(relevant_docs[0], str):\n",
        "            contexts = [str(doc) for doc in relevant_docs]\n\n",
        "        response = rag_system.generate_response(query, relevant_docs)\n\n",
        "        dataset.append({\n",
        "            \"user_input\": query,\n",
        "            \"retrieved_contexts\": contexts,\n",
        "            \"response\": response,\n",
        "            \"reference\": reference,\n",
        "            \"ground_truth\": reference\n",
        "        })\n\n",
        "        print(f\"Query Processed: {query}\")\n",
        "        print(f\"Response: {response}\\n\")\n\n",
        "    evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
        "    return evaluation_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from ragas.metrics import (\n",
        "    faithfulness, context_precision, context_recall, \n",
        "    answer_relevancy, answer_similarity\n",
        ")\n\n",
        "result = evaluate(\n",
        "        dataset=handle_rag_evaluation(), \n",
        "        metrics=[faithfulness, context_precision, context_recall, answer_relevancy, answer_similarity]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from tabulate import tabulate\n\n",
        "df = result.to_pandas()\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
